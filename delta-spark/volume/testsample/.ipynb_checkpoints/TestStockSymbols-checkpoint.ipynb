{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57dd7439-c616-4d28-87b9-f39613454cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/07/23 19:29:28 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststocksymbols` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestStockSymbols\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the table\n",
    "schema = \"symbol STRING, name STRING, price DOUBLE, volume INT\"\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [\n",
    "    (\"AAPL\", \"Apple Inc.\", 175.64, 3000000),\n",
    "    (\"MSFT\", \"Microsoft Corp.\", 341.07, 2500000),\n",
    "    (\"GOOGL\", \"Alphabet Inc.\", 2724.34, 1800000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Delta table in HDFS\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS TestStockSymbols\")\n",
    "\n",
    "# Register the Delta table in Spark SQL catalog\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE TestStockSymbols\n",
    "    USING DELTA\n",
    "    LOCATION 'hdfs://namenode:8020/testlakehouse/TestStockSymbols'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce07909-4e61-4417-a89d-1b9c9bac8f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|teststocksymbols|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current database\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb85fa5-1dc3-4d62-9eb9-1c5a55026b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------+-------+\n",
      "|symbol|           name|  price| volume|\n",
      "+------+---------------+-------+-------+\n",
      "|  MSFT|Microsoft Corp.| 341.07|2500000|\n",
      "| GOOGL|  Alphabet Inc.|2724.34|1800000|\n",
      "|  AAPL|     Apple Inc.| 175.64|3000000|\n",
      "+------+---------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to select all records from the test_table\n",
    "result = spark.sql(\"SELECT * FROM teststocksymbols\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6238a844-ce42-4bc7-9489-68849c7e4af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with additional sample data\n",
    "additional_data = [\n",
    "    (\"TSLA\", \"Tesla Inc.\", 890.10, 2000000),\n",
    "    (\"AMZN\", \"Amazon.com Inc.\", 139.68, 2200000),\n",
    "    (\"NVDA\", \"NVIDIA Corporation\", 585.54, 1500000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "additional_df = spark.createDataFrame(additional_data, schema=schema)\n",
    "\n",
    "# Append data to the Delta table in HDFS\n",
    "additional_df.write.format(\"delta\").mode(\"append\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9f8e57-0c06-4f92-8990-7c1b675594c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------+-------+\n",
      "|symbol|              name|  price| volume|\n",
      "+------+------------------+-------+-------+\n",
      "|  NVDA|NVIDIA Corporation| 585.54|1500000|\n",
      "|  AMZN|   Amazon.com Inc.| 139.68|2200000|\n",
      "|  MSFT|   Microsoft Corp.| 341.07|2500000|\n",
      "| GOOGL|     Alphabet Inc.|2724.34|1800000|\n",
      "|  AAPL|        Apple Inc.| 175.64|3000000|\n",
      "|  TSLA|        Tesla Inc.|  890.1|2000000|\n",
      "+------+------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to select all records from the test_table\n",
    "result = spark.sql(\"SELECT * FROM teststocksymbols\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3850760-b863-4d23-9a7f-c463737f8bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|         symbol|   string|       |\n",
      "|           name|   string|       |\n",
      "|          price|   double|       |\n",
      "|         volume|      int|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table using Spark SQL\n",
    "spark.sql(\"DESCRIBE teststocksymbols\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0c2843-0496-45d5-872a-04eb89263bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/07/23 19:30:33 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststocksymbols` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "+------+------------------+------+-------+\n",
      "|symbol|              name| price| volume|\n",
      "+------+------------------+------+-------+\n",
      "|  AAPL|        Apple Inc.| 74.05|  31001|\n",
      "|  AMZN|   Amazon.com Inc.|224.37|2636527|\n",
      "| GOOGL|     Alphabet Inc.|473.95|1672081|\n",
      "|  MSFT|   Microsoft Corp.|889.79| 896824|\n",
      "|  NVDA|NVIDIA Corporation|693.94|4694680|\n",
      "|  TSLA|        Tesla Inc.| 867.6| 344625|\n",
      "+------+------------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Read Delta table\n",
    "df = spark.read.format(\"delta\").load(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Generate random values for 'price' and 'volume'\n",
    "df_with_random_values = df \\\n",
    "    .withColumn(\"price\", F.round(F.rand() * 1000, 2)) \\\n",
    "    .withColumn(\"volume\", F.round(F.rand() * 5000000).cast(\"int\"))\n",
    "\n",
    "# Overwrite the Delta table with new random values\n",
    "df_with_random_values.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Register the Delta table in Spark SQL catalog again\n",
    "spark.sql(\"DROP TABLE IF EXISTS TestStockSymbols\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE TestStockSymbols\n",
    "    USING DELTA\n",
    "    LOCATION 'hdfs://namenode:8020/testlakehouse/TestStockSymbols'\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result = spark.sql(\"SELECT * FROM TestStockSymbols ORDER BY symbol ASC\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a51b42-93b1-47a5-bcb8-6c92de5ae4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing table: teststocksymbols\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|name           |string   |       |\n",
      "|price          |double   |       |\n",
      "|volume         |int      |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all tables in the current database\n",
    "tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Describe each table\n",
    "for table in tables:\n",
    "    print(f\"Describing table: {table}\")\n",
    "    description = spark.sql(f\"DESCRIBE {table}\")\n",
    "    description.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4dfd5b6-e80e-4207-baff-10dc4f6ae9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current database: default\n"
     ]
    }
   ],
   "source": [
    "current_database = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "print(f\"Current database: {current_database}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af70ccb-99f2-44b5-a942-f4044c1c32b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------+-------+\n",
      "|symbol|              name| price| volume|\n",
      "+------+------------------+------+-------+\n",
      "|  AAPL|        Apple Inc.| 74.05|  31001|\n",
      "|  AMZN|   Amazon.com Inc.|224.37|2636527|\n",
      "| GOOGL|     Alphabet Inc.|473.95|1672081|\n",
      "|  MSFT|   Microsoft Corp.|889.79| 896824|\n",
      "|  NVDA|NVIDIA Corporation|693.94|4694680|\n",
      "|  TSLA|        Tesla Inc.| 867.6| 344625|\n",
      "+------+------------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "result = spark.sql(\"SELECT * FROM TestStockSymbols ORDER BY symbol ASC\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6ef6bda-faaf-4b80-a248-be8a5d2ca197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in /home/NBuser/.local/lib/python3.9/site-packages (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Name: kafka-python\n",
      "Version: 2.0.2\n",
      "Summary: Pure Python client for Apache Kafka\n",
      "Home-page: https://github.com/dpkp/kafka-python\n",
      "Author: Dana Powers\n",
      "Author-email: dana.powers@gmail.com\n",
      "License: Apache License 2.0\n",
      "Location: /home/NBuser/.local/lib/python3.9/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Current sys.path:\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/org.antlr_antlr4-runtime-4.8.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-storage-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-core_2.12-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858\n",
      "/opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "/opt/spark/python\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/usr/lib/python39.zip\n",
      "/usr/lib/python3.9\n",
      "/usr/lib/python3.9/lib-dynload\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "'/home/NBuser/.local/lib/python3.9/site-packages' is already in sys.path\n",
      "Updated sys.path:\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/org.antlr_antlr4-runtime-4.8.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-storage-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-core_2.12-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858\n",
      "/opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "/opt/spark/python\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/usr/lib/python39.zip\n",
      "/usr/lib/python3.9\n",
      "/usr/lib/python3.9/lib-dynload\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip show kafka-python\n",
    "# Location: /home/NBuser/.local/lib/python3.9/site-packages\n",
    "\n",
    "import sys\n",
    "\n",
    "# Print current sys.path\n",
    "print(\"Current sys.path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "\n",
    "# Path to append\n",
    "new_path = '/home/NBuser/.local/lib/python3.9/site-packages'\n",
    "\n",
    "# Append path if it's not already present\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)\n",
    "    print(f\"Added '{new_path}' to sys.path\")\n",
    "else:\n",
    "    print(f\"'{new_path}' is already in sys.path\")\n",
    "\n",
    "# Verify updated sys.path\n",
    "print(\"Updated sys.path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0238fba-006c-42f4-9215-eac928c7d1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (52) Empty reply from server\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET http://172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a8308e8-9432-4e89-a957-d480d8d322ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 1: kafka-topics.sh: command not found\n"
     ]
    }
   ],
   "source": [
    "!kafka-topics.sh --list --bootstrap-server 172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d49b718b-9b5c-4e79-b5e4-3dba1be1040c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n"
     ]
    },
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m kafka_bootstrap_servers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Adjust this to your Kafka broker's address\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize Kafka admin client\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m admin_client \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaAdminClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkafka_bootstrap_servers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Get the list of topics\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/admin/client.py:208\u001b[0m, in \u001b[0;36mKafkaAdminClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    205\u001b[0m reporters \u001b[38;5;241m=\u001b[39m [reporter() \u001b[38;5;28;01mfor\u001b[39;00m reporter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_reporters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics \u001b[38;5;241m=\u001b[39m Metrics(metric_config, reporters)\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmetric_group_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcheck_version(timeout\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version_auto_timeout_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Get auto-discovered version from client if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/client_async.py:244\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     check_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version_auto_timeout_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/client_async.py:900\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m--> 900\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoBrokersAvailable()\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_connect(try_node)\n\u001b[1;32m    902\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conns[try_node]\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaConsumer\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"  # Adjust this to your Kafka broker's address\n",
    "\n",
    "# Initialize Kafka admin client\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=kafka_bootstrap_servers)\n",
    "\n",
    "# Get the list of topics\n",
    "try:\n",
    "    topics = admin_client.list_topics()\n",
    "    print(f\"Topics available: {topics}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Kafka: {e}\")\n",
    "\n",
    "# Initialize Kafka consumer to check topic data (optional)\n",
    "consumer = KafkaConsumer(bootstrap_servers=kafka_bootstrap_servers)\n",
    "\n",
    "# Check if specific topic exists\n",
    "topic = \"phongdinhcstest\"  # Replace with your topic\n",
    "if topic in consumer.topics():\n",
    "    print(f\"Topic '{topic}' exists.\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdb681-0f98-4af8-9aac-d6b140c018a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(bootstrap_servers=kafka_bootstrap_servers)\n",
    "\n",
    "# Send a test message\n",
    "try:\n",
    "    producer.send('phongdinhcstest', key=b'key', value=b'test message')\n",
    "    producer.flush()\n",
    "    print(\"Test message sent successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to send message: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbc993-3e49-462f-a156-1eed3be5043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Initialize Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'phongdinhcstest',\n",
    "    bootstrap_servers=kafka_bootstrap_servers,\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "# Poll for messages\n",
    "try:\n",
    "    for message in consumer:\n",
    "        print(f\"Received message: {message.value.decode('utf-8')}\")\n",
    "        break  # Stop after receiving the first message\n",
    "except Exception as e:\n",
    "    print(f\"Failed to receive message: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c3d4c-66d2-4373-821f-47be1101fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkIntegration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Kafka parameters\n",
    "kafka_bootstrap_servers = \"kafka:9092\"\n",
    "topic = \"phongdinhcstest\"\n",
    "\n",
    "# Read data from Kafka topic\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "# Extract the value column and convert it to string\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Write the data to console\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
