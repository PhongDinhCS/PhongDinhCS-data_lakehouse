{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57dd7439-c616-4d28-87b9-f39613454cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/07/23 19:29:28 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststocksymbols` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestStockSymbols\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the table\n",
    "schema = \"symbol STRING, name STRING, price DOUBLE, volume INT\"\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [\n",
    "    (\"AAPL\", \"Apple Inc.\", 175.64, 3000000),\n",
    "    (\"MSFT\", \"Microsoft Corp.\", 341.07, 2500000),\n",
    "    (\"GOOGL\", \"Alphabet Inc.\", 2724.34, 1800000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Delta table in HDFS\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS TestStockSymbols\")\n",
    "\n",
    "# Register the Delta table in Spark SQL catalog\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE TestStockSymbols\n",
    "    USING DELTA\n",
    "    LOCATION 'hdfs://namenode:8020/testlakehouse/TestStockSymbols'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce07909-4e61-4417-a89d-1b9c9bac8f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|teststocksymbols|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current database\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb85fa5-1dc3-4d62-9eb9-1c5a55026b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------+-------+\n",
      "|symbol|           name|  price| volume|\n",
      "+------+---------------+-------+-------+\n",
      "|  MSFT|Microsoft Corp.| 341.07|2500000|\n",
      "| GOOGL|  Alphabet Inc.|2724.34|1800000|\n",
      "|  AAPL|     Apple Inc.| 175.64|3000000|\n",
      "+------+---------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to select all records from the test_table\n",
    "result = spark.sql(\"SELECT * FROM teststocksymbols\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6238a844-ce42-4bc7-9489-68849c7e4af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with additional sample data\n",
    "additional_data = [\n",
    "    (\"TSLA\", \"Tesla Inc.\", 890.10, 2000000),\n",
    "    (\"AMZN\", \"Amazon.com Inc.\", 139.68, 2200000),\n",
    "    (\"NVDA\", \"NVIDIA Corporation\", 585.54, 1500000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "additional_df = spark.createDataFrame(additional_data, schema=schema)\n",
    "\n",
    "# Append data to the Delta table in HDFS\n",
    "additional_df.write.format(\"delta\").mode(\"append\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9f8e57-0c06-4f92-8990-7c1b675594c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------+-------+\n",
      "|symbol|              name|  price| volume|\n",
      "+------+------------------+-------+-------+\n",
      "|  NVDA|NVIDIA Corporation| 585.54|1500000|\n",
      "|  AMZN|   Amazon.com Inc.| 139.68|2200000|\n",
      "|  MSFT|   Microsoft Corp.| 341.07|2500000|\n",
      "| GOOGL|     Alphabet Inc.|2724.34|1800000|\n",
      "|  AAPL|        Apple Inc.| 175.64|3000000|\n",
      "|  TSLA|        Tesla Inc.|  890.1|2000000|\n",
      "+------+------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to select all records from the test_table\n",
    "result = spark.sql(\"SELECT * FROM teststocksymbols\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3850760-b863-4d23-9a7f-c463737f8bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|         symbol|   string|       |\n",
      "|           name|   string|       |\n",
      "|          price|   double|       |\n",
      "|         volume|      int|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table using Spark SQL\n",
    "spark.sql(\"DESCRIBE teststocksymbols\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0c2843-0496-45d5-872a-04eb89263bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/07/23 19:30:33 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststocksymbols` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "+------+------------------+------+-------+\n",
      "|symbol|              name| price| volume|\n",
      "+------+------------------+------+-------+\n",
      "|  AAPL|        Apple Inc.| 74.05|  31001|\n",
      "|  AMZN|   Amazon.com Inc.|224.37|2636527|\n",
      "| GOOGL|     Alphabet Inc.|473.95|1672081|\n",
      "|  MSFT|   Microsoft Corp.|889.79| 896824|\n",
      "|  NVDA|NVIDIA Corporation|693.94|4694680|\n",
      "|  TSLA|        Tesla Inc.| 867.6| 344625|\n",
      "+------+------------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Read Delta table\n",
    "df = spark.read.format(\"delta\").load(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Generate random values for 'price' and 'volume'\n",
    "df_with_random_values = df \\\n",
    "    .withColumn(\"price\", F.round(F.rand() * 1000, 2)) \\\n",
    "    .withColumn(\"volume\", F.round(F.rand() * 5000000).cast(\"int\"))\n",
    "\n",
    "# Overwrite the Delta table with new random values\n",
    "df_with_random_values.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Register the Delta table in Spark SQL catalog again\n",
    "spark.sql(\"DROP TABLE IF EXISTS TestStockSymbols\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE TestStockSymbols\n",
    "    USING DELTA\n",
    "    LOCATION 'hdfs://namenode:8020/testlakehouse/TestStockSymbols'\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result = spark.sql(\"SELECT * FROM TestStockSymbols ORDER BY symbol ASC\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a51b42-93b1-47a5-bcb8-6c92de5ae4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing table: teststocksymbols\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|name           |string   |       |\n",
      "|price          |double   |       |\n",
      "|volume         |int      |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all tables in the current database\n",
    "tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Describe each table\n",
    "for table in tables:\n",
    "    print(f\"Describing table: {table}\")\n",
    "    description = spark.sql(f\"DESCRIBE {table}\")\n",
    "    description.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4dfd5b6-e80e-4207-baff-10dc4f6ae9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current database: default\n"
     ]
    }
   ],
   "source": [
    "current_database = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "print(f\"Current database: {current_database}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af70ccb-99f2-44b5-a942-f4044c1c32b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------+-------+\n",
      "|symbol|              name| price| volume|\n",
      "+------+------------------+------+-------+\n",
      "|  AAPL|        Apple Inc.| 74.05|  31001|\n",
      "|  AMZN|   Amazon.com Inc.|224.37|2636527|\n",
      "| GOOGL|     Alphabet Inc.|473.95|1672081|\n",
      "|  MSFT|   Microsoft Corp.|889.79| 896824|\n",
      "|  NVDA|NVIDIA Corporation|693.94|4694680|\n",
      "|  TSLA|        Tesla Inc.| 867.6| 344625|\n",
      "+------+------------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "result = spark.sql(\"SELECT * FROM TestStockSymbols ORDER BY symbol ASC\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6ef6bda-faaf-4b80-a248-be8a5d2ca197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in /home/NBuser/.local/lib/python3.9/site-packages (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Name: kafka-python\n",
      "Version: 2.0.2\n",
      "Summary: Pure Python client for Apache Kafka\n",
      "Home-page: https://github.com/dpkp/kafka-python\n",
      "Author: Dana Powers\n",
      "Author-email: dana.powers@gmail.com\n",
      "License: Apache License 2.0\n",
      "Location: /home/NBuser/.local/lib/python3.9/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Current sys.path:\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/org.antlr_antlr4-runtime-4.8.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-storage-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-core_2.12-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858\n",
      "/opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "/opt/spark/python\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/usr/lib/python39.zip\n",
      "/usr/lib/python3.9\n",
      "/usr/lib/python3.9/lib-dynload\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "'/home/NBuser/.local/lib/python3.9/site-packages' is already in sys.path\n",
      "Updated sys.path:\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/org.antlr_antlr4-runtime-4.8.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-storage-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858/io.delta_delta-core_2.12-2.3.0.jar\n",
      "/tmp/spark-dcbe9d73-eae7-4d31-8a2c-1fc4e910ff77/userFiles-16e66e1c-b01f-48c3-aae5-72f5dfecb858\n",
      "/opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "/opt/spark/python\n",
      "/opt/spark/work-dir/volume/testsample\n",
      "/usr/lib/python39.zip\n",
      "/usr/lib/python3.9\n",
      "/usr/lib/python3.9/lib-dynload\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n",
      "/home/NBuser/.local/lib/python3.9/site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip show kafka-python\n",
    "# Location: /home/NBuser/.local/lib/python3.9/site-packages\n",
    "\n",
    "import sys\n",
    "\n",
    "# Print current sys.path\n",
    "print(\"Current sys.path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "\n",
    "# Path to append\n",
    "new_path = '/home/NBuser/.local/lib/python3.9/site-packages'\n",
    "\n",
    "# Append path if it's not already present\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)\n",
    "    print(f\"Added '{new_path}' to sys.path\")\n",
    "else:\n",
    "    print(f\"'{new_path}' is already in sys.path\")\n",
    "\n",
    "# Verify updated sys.path\n",
    "print(\"Updated sys.path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a07f74-ad30-4014-b65c-b9911f42a79c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/07/28 08:46:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaDeltaIntegration\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d69e315-b92e-485b-9599-3c1df56ebdea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": " Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kafka_stream \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m172.18.0.99:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphongdinhcs-test-topic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert Kafka data to String\u001b[39;00m\n\u001b[1;32m      8\u001b[0m kafka_stream \u001b[38;5;241m=\u001b[39m kafka_stream\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming.py:469\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator:Heartbeat poll expired, leaving group\n"
     ]
    }
   ],
   "source": [
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"172.18.0.99:9092\") \\\n",
    "    .option(\"subscribe\", \"phongdinhcs-test-topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert Kafka data to String\n",
    "kafka_stream = kafka_stream.selectExpr(\"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0238fba-006c-42f4-9215-eac928c7d1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (52) Empty reply from server\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET http://172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2f7a92-7c1c-406b-92ed-8f19bdcf9bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to 172.18.0.99 9092 port [tcp/*] succeeded!\n",
      "Connection to 172.18.0.99 9093 port [tcp/*] succeeded!\n"
     ]
    }
   ],
   "source": [
    "!nc -zv 172.18.0.99 9092\n",
    "!nc -zv 172.18.0.99 9093\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8308e8-9432-4e89-a957-d480d8d322ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__consumer_offsets\n",
      "phongdinhcs-test-topic\n"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server 172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17961b-b8f0-4eee-89b6-2e717b0cc516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!/opt/kafka/bin/kafka-topics.sh --create --topic phongdinhcs-test-topic --bootstrap-server 172.18.0.99:9092 --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242bd3ca-b1cd-4b23-a0d0-59da0378d738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__consumer_offsets\n",
      "phongdinhcs-test-topic\n"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server 172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452be777-fd9c-4289-a533-69c8e9b26286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consumer group 'my-group' has no active members.\n",
      "\n",
      "GROUP           TOPIC                  PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\n",
      "my-group        phongdinhcs-test-topic 0          1               1               0               -               -               -\n"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server 172.18.0.99:9092 --describe --group my-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e51531-30f9-448b-9892-a4d6f39fb5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message sent to topic 'phongdinhcs-test-topic'\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='172.18.0.99:9092',\n",
    "    value_serializer=lambda v: str(v).encode('utf-8')  # Convert messages to bytes\n",
    ")\n",
    "\n",
    "# Send a message to the topic\n",
    "producer.send('phongdinhcs-test-topic', value='Hello, Kafka!')\n",
    "\n",
    "# Wait for all messages to be sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message sent to topic 'phongdinhcs-test-topic'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c0ac2b-8c82-49ed-a400-4dd1c5978075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: Hello, Kafka!\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'phongdinhcs-test-topic',\n",
    "    bootstrap_servers='172.18.0.99:9092',\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: x.decode('utf-8')  # Decode bytes to string\n",
    ")\n",
    "\n",
    "# Consume messages from the topic\n",
    "for message in consumer:\n",
    "    print(f\"Received message: {message.value}\")\n",
    "    break  # Remove this break to continuously consume messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890b4b6-a814-48f0-8429-31fa93cd7bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
