{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bf27a3-48c5-47b8-91b1-7fd6a3ca5e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|teststocksymbols|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current database\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0567d718-3284-4687-bba1-b9cada7f6612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/12 22:46:07 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststocksymbols` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "+------+------------------+\n",
      "|symbol|              name|\n",
      "+------+------------------+\n",
      "|  NVDA|NVIDIA Corporation|\n",
      "|  MSFT|   Microsoft Corp.|\n",
      "|  AMZN|   Amazon.com Inc.|\n",
      "| GOOGL|     Alphabet Inc.|\n",
      "|  TSLA|        Tesla Inc.|\n",
      "|  AAPL|        Apple Inc.|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestStockSymbols\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the new schema for the table\n",
    "schema = \"symbol STRING, name STRING\"\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [\n",
    "    (\"AAPL\", \"Apple Inc.\"),\n",
    "    (\"MSFT\", \"Microsoft Corp.\"),\n",
    "    (\"GOOGL\", \"Alphabet Inc.\"),\n",
    "    (\"TSLA\", \"Tesla Inc.\"),\n",
    "    (\"AMZN\", \"Amazon.com Inc.\"),\n",
    "    (\"NVDA\", \"NVIDIA Corporation\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Delta table in HDFS\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs://namenode:8020/testlakehouse/TestStockSymbols\")\n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS TestStockSymbols\")\n",
    "\n",
    "# Register the updated Delta table in Spark SQL catalog\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE TestStockSymbols\n",
    "    USING DELTA\n",
    "    LOCATION 'hdfs://namenode:8020/testlakehouse/TestStockSymbols'\n",
    "\"\"\")\n",
    "\n",
    "# Query to select all records from the test_table\n",
    "result = spark.sql(\"SELECT * FROM teststocksymbols\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dce07909-4e61-4417-a89d-1b9c9bac8f81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|teststockcurrentp...|      false|\n",
      "|  default|    teststocksymbols|      false|\n",
      "|  default|teststocktransaction|      false|\n",
      "|         |     stock_news_html|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current database\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4dfd5b6-e80e-4207-baff-10dc4f6ae9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current database: default\n"
     ]
    }
   ],
   "source": [
    "current_database = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "print(f\"Current database: {current_database}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558dfe62-9049-4f0a-801e-7d5d333f9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a94b9b98-4ce3-4d6d-871d-c9a03d89bec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/12 22:30:06 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststockcurrentprice` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "Table TestStockCurrentPrice created and registered successfully.\n",
      "+------+------+---------+\n",
      "|symbol|price |volume   |\n",
      "+------+------+---------+\n",
      "|GOOGL |185.5 |12760100 |\n",
      "|MSFT  |193.02|33994710 |\n",
      "|AMZN  |126.36|214769500|\n",
      "|NVDA  |449.52|17175680 |\n",
      "|TSLA  |256.56|126332500|\n",
      "|AAPL  |234.82|43234280 |\n",
      "+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestStockCurrentPrice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the table\n",
    "schema = \"symbol STRING, price DOUBLE, volume LONG\"\n",
    "\n",
    "# Sample data to create the DataFrame\n",
    "data = [\n",
    "    (\"AAPL\", 234.82, 43234280),\n",
    "    (\"MSFT\", 193.02, 33994710),\n",
    "    (\"GOOGL\",185.5, 12760100),\n",
    "    (\"NVDA\", 449.52, 17175680),\n",
    "    (\"AMZN\", 126.36, 214769500),\n",
    "    (\"TSLA\", 256.56, 126332500)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Define the path for the Delta table\n",
    "delta_path = \"hdfs://namenode:8020/testlakehouse/TestStockCurrentPrice\"\n",
    "\n",
    "# Drop the existing Delta table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS TestStockCurrentPrice\")\n",
    "\n",
    "# Write DataFrame to Delta table in HDFS\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Create the Delta table in Spark SQL catalog\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE TestStockCurrentPrice\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table TestStockCurrentPrice created and registered successfully.\")\n",
    "\n",
    "# Query some data\n",
    "data_preview = spark.sql(\"SELECT * FROM TestStockCurrentPrice LIMIT 6\")\n",
    "data_preview.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae8e69ff-a126-4aa8-8c3c-110e15f1469f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/12 22:27:05 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`teststocktransaction` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "Delta table TestStockTransaction created and registered successfully.\n",
      "+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+--------------------------+\n",
      "|AAPL_price|AAPL_volume|AMZN_price|AMZN_volume|GOOG_price|GOOG_volume|MSFT_price|MSFT_volume|NVDA_price|NVDA_volume|TSLA_price|TSLA_volume|timestamp                 |\n",
      "+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+--------------------------+\n",
      "|234.82    |43234280   |126.36    |214769500  |185.5     |12760100   |193.02    |33994710   |449.52    |17175680   |256.56    |126332500  |2024-08-12 22:27:03.951914|\n",
      "+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+----------+-----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreateTestStockTransaction\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = \"\"\"\n",
    "    AAPL_price DOUBLE,\n",
    "    AAPL_volume LONG,\n",
    "    AMZN_price DOUBLE,\n",
    "    AMZN_volume LONG,\n",
    "    GOOG_price DOUBLE,\n",
    "    GOOG_volume LONG,\n",
    "    MSFT_price DOUBLE,\n",
    "    MSFT_volume LONG,\n",
    "    NVDA_price DOUBLE,\n",
    "    NVDA_volume LONG,\n",
    "    TSLA_price DOUBLE,\n",
    "    TSLA_volume LONG\n",
    "\"\"\"\n",
    "\n",
    "# Sample data to create the DataFrame\n",
    "data = [\n",
    "    (234.82, 43234280, 126.36, 214769500, 185.5, 12760100, 193.02, 33994710, 449.52, 17175680, 256.56, 126332500)\n",
    "]\n",
    "\n",
    "# Create DataFrame with the sample data\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Add the timestamp column\n",
    "df_with_timestamp = df.withColumn(\"timestamp\", F.current_timestamp())\n",
    "\n",
    "# Define the path for the Delta table\n",
    "delta_path = \"hdfs://namenode:8020/testlakehouse/TestStockTransaction\"\n",
    "\n",
    "# Write DataFrame to Delta table in HDFS\n",
    "df_with_timestamp.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS TestStockTransaction\")\n",
    "\n",
    "# Register the Delta table in Spark SQL catalog\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE TestStockTransaction\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Delta table TestStockTransaction created and registered successfully.\")\n",
    "\n",
    "# Query some data to confirm\n",
    "data_preview = spark.sql(\"SELECT * FROM TestStockTransaction LIMIT 6\")\n",
    "data_preview.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaae0a-e1ba-4816-b32f-e3b0b38f2add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AppendDataToTestStockTransaction\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the Delta path for TestStockTransaction\n",
    "delta_path_TestStockTransaction = \"hdfs://namenode:8020/testlakehouse/TestStockTransaction\"\n",
    "\n",
    "# Connect to PostgreSQL and fetch a random row\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"bqnbcj8kxuogsigyhnzy\",\n",
    "        user=\"ufjklpchveyybgraqhxu\",\n",
    "        password=\"AyR5dzFuySPaAcWd5po1AJMK063nkG\",\n",
    "        host=\"bqnbcj8kxuogsigyhnzy-postgresql.services.clever-cloud.com\",\n",
    "        port=\"50013\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Fetch a random row from PostgreSQL\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT * FROM sample_stock_data ORDER BY RANDOM() LIMIT 1\n",
    "    \"\"\")\n",
    "\n",
    "    # Fetch the row\n",
    "    random_row = cur.fetchone()\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Define the columns\n",
    "columns = [\n",
    "    'AAPL_price', 'AAPL_volume', 'AMZN_price', 'AMZN_volume', \n",
    "    'GOOG_price', 'GOOG_volume', 'MSFT_price', 'MSFT_volume',\n",
    "    'NVDA_price', 'NVDA_volume', 'TSLA_price', 'TSLA_volume'\n",
    "]\n",
    "\n",
    "# Convert the row to a DataFrame\n",
    "row_df = pd.DataFrame([random_row], columns=columns)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "row_spark_df = spark.createDataFrame(row_df)\n",
    "\n",
    "# Convert the data types\n",
    "row_spark_df_corrected = row_spark_df \\\n",
    "    .withColumn(\"AAPL_price\", col(\"AAPL_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"AAPL_volume\", col(\"AAPL_volume\").cast(\"long\")) \\\n",
    "    .withColumn(\"AMZN_price\", col(\"AMZN_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"AMZN_volume\", col(\"AMZN_volume\").cast(\"long\")) \\\n",
    "    .withColumn(\"GOOG_price\", col(\"GOOG_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"GOOG_volume\", col(\"GOOG_volume\").cast(\"long\")) \\\n",
    "    .withColumn(\"MSFT_price\", col(\"MSFT_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"MSFT_volume\", col(\"MSFT_volume\").cast(\"long\")) \\\n",
    "    .withColumn(\"NVDA_price\", col(\"NVDA_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"NVDA_volume\", col(\"NVDA_volume\").cast(\"long\")) \\\n",
    "    .withColumn(\"TSLA_price\", col(\"TSLA_price\").cast(\"double\")) \\\n",
    "    .withColumn(\"TSLA_volume\", col(\"TSLA_volume\").cast(\"long\"))\n",
    "\n",
    "# Add the timestamp column\n",
    "row_spark_df_with_timestamp = row_spark_df_corrected.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# Append data to the TestStockTransaction Delta table\n",
    "row_spark_df_with_timestamp.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_path_TestStockTransaction)\n",
    "\n",
    "print(\"Data appended to TestStockTransaction successfully.\")\n",
    "\n",
    "# Query some data to confirm\n",
    "data_preview = spark.sql(\"SELECT * FROM TestStockTransaction LIMIT 6\")\n",
    "data_preview.show(truncate=False)\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "# Define the path for the Delta table\n",
    "delta_path_TestStockCurrentPrice = \"hdfs://namenode:8020/testlakehouse/TestStockCurrentPrice\"\n",
    "\n",
    "\n",
    "# Transform the DataFrame from wide to long format\n",
    "melted_df = row_spark_df.selectExpr(\n",
    "    \"stack(6, 'AAPL', AAPL_price, 'AMZN', AMZN_price, 'GOOG', GOOG_price, 'MSFT', MSFT_price, 'NVDA', NVDA_price, 'TSLA', TSLA_price) as (symbol, price)\"\n",
    ").join(\n",
    "    row_spark_df.selectExpr(\n",
    "        \"stack(6, 'AAPL', AAPL_volume, 'AMZN', AMZN_volume, 'GOOG', GOOG_volume, 'MSFT', MSFT_volume, 'NVDA', NVDA_volume, 'TSLA', TSLA_volume) as (symbol, volume)\"\n",
    "    ),\n",
    "    on=\"symbol\"\n",
    ").select(\"symbol\", \"price\", \"volume\")\n",
    "\n",
    "# Convert columns to the appropriate data types\n",
    "melted_df_corrected = melted_df \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .withColumn(\"volume\", col(\"volume\").cast(\"long\"))\n",
    "\n",
    "# Write the corrected DataFrame to the Delta table\n",
    "melted_df_corrected.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_path_TestStockCurrentPrice)\n",
    "\n",
    "print(\"Delta table updated TestStockCurrentPrice successfully.\")\n",
    "# Query some data to confirm\n",
    "data_preview = spark.sql(\"SELECT * FROM TestStockCurrentPrice LIMIT 6\")\n",
    "data_preview.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd9a5ff0-0e63-4f15-bbe4-af687289676e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|price          |double   |       |\n",
      "|volume         |bigint   |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "+------------+\n",
      "|record_count|\n",
      "+------------+\n",
      "|6           |\n",
      "+------------+\n",
      "\n",
      "+------+------+---------+\n",
      "|symbol|price |volume   |\n",
      "+------+------+---------+\n",
      "|AAPL  |209.27|119548600|\n",
      "|AMZN  |161.02|83149440 |\n",
      "|GOOG  |160.64|34907820 |\n",
      "|MSFT  |395.15|40709240 |\n",
      "|NVDA  |100.45|552842400|\n",
      "|TSLA  |198.88|100308800|\n",
      "+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table schema\n",
    "schema = spark.sql(\"DESCRIBE TestStockCurrentPrice\")\n",
    "schema.show(truncate=False)\n",
    "\n",
    "# Count the number of rows in the table\n",
    "row_count = spark.sql(\"SELECT COUNT(*) AS record_count FROM TestStockCurrentPrice\")\n",
    "row_count.show(truncate=False)\n",
    "\n",
    "# Query some data\n",
    "data_preview = spark.sql(\"SELECT * FROM TestStockCurrentPrice LIMIT 6\")\n",
    "data_preview.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49651605-ab51-4bce-8c48-ae49cf017579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the key words appear on the news paper: {'AAPL': 0, 'AMZN': 0, 'GOOG': 0, 'MSFT': 0, 'NVDA': 0, 'TSLA': 0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a request to the URL\n",
    "url = 'https://seekingalpha.com/market-news'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the text from the entire HTML\n",
    "text = soup.get_text()\n",
    "\n",
    "# List of keywords to count\n",
    "keywords = ['AAPL', 'AMZN', 'GOOG', 'MSFT', 'NVDA', 'TSLA']\n",
    "\n",
    "# Count occurrences of each keyword\n",
    "keyword_counts = {keyword: text.upper().count(keyword) for keyword in keywords}\n",
    "\n",
    "print(\"Count the key words appear on the news paper:\",keyword_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbc0fdb2-5d45-4b07-929c-c7fcb1535418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML text pushed to Kafka topic 'phongdinhcs-test-topic' at 2024-08-12 20:27:22\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime\n",
    "\n",
    "# Make a request to the URL\n",
    "url = 'https://seekingalpha.com/market-news'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the text from the entire HTML\n",
    "text = soup.get_text()\n",
    "\n",
    "# Kafka configuration\n",
    "bootstrap_servers = '172.18.0.99:9092'\n",
    "topic = 'phongdinhcs-test-topic'\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    value_serializer=lambda v: str(v).encode('utf-8')  # Convert messages to bytes\n",
    ")\n",
    "\n",
    "# Send the HTML text to the Kafka topic\n",
    "producer.send(topic, value=text)\n",
    "\n",
    "# Wait for all messages to be sent\n",
    "producer.flush()\n",
    "\n",
    "# Get current date and time\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f\"HTML text pushed to Kafka topic 'phongdinhcs-test-topic' at {now}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49debca4-757d-4fa6-a537-d1bd0867301a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"timestamp\": \"2024-08-12 00:49:23\",\n",
      "    \"AAPL\": 0,\n",
      "    \"AMZN\": 0,\n",
      "    \"GOOG\": 2,\n",
      "    \"MSFT\": 0,\n",
      "    \"NVDA\": 0,\n",
      "    \"TSLA\": 0,\n",
      "    \"hdfs_location\": \"hdfs://namenode:8020/testlakehouse/stock_news_html\",\n",
      "    \"crawl_from_website\": \"\\\"https://seekingalpha.com/market-news\\\"\",\n",
      "    \"html_text\": \"' surpasses $1B worldwideAMC -1.57%Yesterday, 5:20 PM15 CommentsNew York City, Dallas lead apartment construction boom in 2024 - reportEQR +1.36%Yesterday, 4:18 PM2 CommentsGoldman screens for post-selloff stocks with healthy fundamentals at a discountIWB +0.47%Yesterday, 4:04 PM15 CommentsSA Asks: Will the Fed do an emergency rate cut?AGG +0.35%Yesterday, 3:29 PM59 CommentsTrump says Microsoft told him Iran was behind campaign hackDJT -0.49%Yesterday, 1:11 PM175 CommentsDisney unveils 'Avatar', 'Indiana Jones', 'Monsters, Inc' and 'Cars' attractions in big parks and resort pushDIS +0.29%Yesterday, 11:04 AM7 CommentsMarketWise founder Porter Stansberry resigns as CEO, ChairmanMKTW +5.88%Yesterday, 10:23 AM13 CommentsHard hats or party hats? Walmart earnings could shake up the retail sectorWMT +0.43%Yesterday, 9:02 AM4 CommentsLeisure vehicles about to get a Fed bumpWGO -0.60%Yesterday, 9:00 AM6 CommentsEinhorn's Greenlight says it won't matter who wins the US election for marketsTPR +0.79%Yesterday, 9:00 AM22 CommentsEarnings Week Ahead: Walmart, Home Depot, Alibaba, Cisco and moreBABA -0.40%Yesterday, 8:01 AM1 CommentCan Perplexity AI wrest power from search engine goliath Google?GOOG +0.95%Yesterday, 8:00 AM17 CommentsGold rally may not be over yet, to peak at $2,450 in Q4, ING saysGLD +0.25%Sat, Aug. 1021 CommentsCitigroup to sell trust business amid turnaround effortsC -0.28%Sat, Aug. 104 CommentsExecutive reshuffles: Uranium Energy, Nutrien and Logitech International Companies in focusLOGI +0.75%Sat, Aug. 102 CommentsRate cuts may impact big banks more negatively than regionals, analyst saysBAC +0.18%Sat, Aug. 1017 CommentsThe U.S. uninsured rate hits 8%, a post-pandemic highUNH -1.32%Sat, Aug. 10144 CommentsPiper lists eight defensive biotechs to face market volatilityALNY -0.35%Sat, Aug. 105 CommentsFed Governor Bowman sees inflation declining further, cautions against 'overreacting'Sat, Aug. 1017 CommentsNotable analyst calls this week: Disney, Super Micro and Amgen among top picksSP500 +0.47%Sat, Aug. 10Earnings help Axon to top industrial gainer of week, while dragging down AtkoreAXON -0.94%Sat, Aug. 10Disney unveils new 'Incredibles' film, gives updates on 'Moana', 'Avatar'DIS +0.29%Sat, Aug. 1033 CommentsReal estate stocks close a volatile week roughly flatXLRE +0.50%Sat, Aug. 101 CommentStellantis to cut up to 2,450 workers at Michigan plant as Old Ram 1500 discontinuedSTLA -1.35%Sat, Aug. 1022 Comments4 out of 6 communications services stocks deliver EPS wins this week: Earnings ScorecardXLC +0.85%Sat, Aug. 1012345NextSeeking Alpha - Power to InvestorsPower to InvestorsFollow usDownload appSubscription Support: 1-347-509-6837RSS FeedSitemapGroup SubscriptionsAffiliate ProgramAbout UsCareersContact UsAccountAccount LoginCreate PortfolioManage My PortfolioPortfolio Health CheckPrivacyAlert PreferencesSubscriptionsPremium & ProGroup SubscriptionsAlpha PicksAbout Alpha PicksFREE NewslettersInvesting GroupsLearn About Investing GroupsMos\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring, to_json, struct\n",
    "import json\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadDeltaTable\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table in HDFS\n",
    "delta_table_path = \"hdfs://namenode:8020/testlakehouse/stock_news_html\"\n",
    "\n",
    "# Read the Delta table into a DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Determine the last column name\n",
    "last_column_name = df.columns[-1]\n",
    "\n",
    "# Truncate the last column to characters 2000 to 3000\n",
    "truncated_df = df.select(\n",
    "    *[\n",
    "        (substring(col(column_name), 2000, 3000).alias(column_name) if column_name == last_column_name else col(column_name))\n",
    "        for column_name in df.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Convert the DataFrame to JSON format\n",
    "json_df = truncated_df.select(to_json(struct(*truncated_df.columns)).alias(\"json\"))\n",
    "\n",
    "# Collect the JSON data\n",
    "json_data = json_df.collect()\n",
    "\n",
    "# Print the JSON data in a readable format\n",
    "for row in json_data:\n",
    "    print(json.dumps(json.loads(row[\"json\"]), indent=4))\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b53b323-1c45-4148-aa42-03e44c2f23ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing table: teststockcurrentprice\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|price          |double   |       |\n",
      "|volume         |bigint   |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "+------------+\n",
      "|record_count|\n",
      "+------------+\n",
      "|6           |\n",
      "+------------+\n",
      "\n",
      "Describing table: teststocksymbols\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|name           |string   |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "+------------+\n",
      "|record_count|\n",
      "+------------+\n",
      "|3           |\n",
      "+------------+\n",
      "\n",
      "Describing table: teststocktransaction\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|AAPL_price     |double   |       |\n",
      "|AAPL_volume    |bigint   |       |\n",
      "|AMZN_price     |double   |       |\n",
      "|AMZN_volume    |bigint   |       |\n",
      "|GOOG_price     |double   |       |\n",
      "|GOOG_volume    |bigint   |       |\n",
      "|MSFT_price     |double   |       |\n",
      "|MSFT_volume    |bigint   |       |\n",
      "|NVDA_price     |double   |       |\n",
      "|NVDA_volume    |bigint   |       |\n",
      "|TSLA_price     |double   |       |\n",
      "|TSLA_volume    |bigint   |       |\n",
      "|timestamp      |timestamp|       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "+------------+\n",
      "|record_count|\n",
      "+------------+\n",
      "|4           |\n",
      "+------------+\n",
      "\n",
      "Describing table: stock_news_html\n",
      "+------------------+---------+-------+\n",
      "|col_name          |data_type|comment|\n",
      "+------------------+---------+-------+\n",
      "|timestamp         |string   |null   |\n",
      "|AAPL              |bigint   |null   |\n",
      "|AMZN              |bigint   |null   |\n",
      "|GOOG              |bigint   |null   |\n",
      "|MSFT              |bigint   |null   |\n",
      "|NVDA              |bigint   |null   |\n",
      "|TSLA              |bigint   |null   |\n",
      "|hdfs_location     |string   |null   |\n",
      "|crawl_from_website|string   |null   |\n",
      "|html_text         |string   |null   |\n",
      "+------------------+---------+-------+\n",
      "\n",
      "+------------+\n",
      "|record_count|\n",
      "+------------+\n",
      "|1           |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view for the Delta table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW stock_news_html AS\n",
    "    SELECT * FROM delta.`hdfs://namenode:8020/testlakehouse/stock_news_html`\n",
    "\"\"\")\n",
    "\n",
    "# Show all tables in the current database\n",
    "tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Describe each table and show the record count\n",
    "for table in tables:\n",
    "    print(f\"Describing table: {table}\")\n",
    "    \n",
    "    # Describe the table\n",
    "    description = spark.sql(f\"DESCRIBE {table}\")\n",
    "    description.show(truncate=False)\n",
    "    \n",
    "    # Count the number of records in the table\n",
    "    record_count = spark.sql(f\"SELECT COUNT(*) AS record_count FROM {table}\")\n",
    "    record_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b55b3e0d-52c3-48c7-9e52-0bed4e202900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing table: teststockcurrentprice\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|price          |double   |       |\n",
      "|volume         |bigint   |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "Describing table: teststocksymbols\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|symbol         |string   |       |\n",
      "|name           |string   |       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "Describing table: teststocktransaction\n",
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|AAPL_price     |double   |       |\n",
      "|AAPL_volume    |bigint   |       |\n",
      "|AMZN_price     |double   |       |\n",
      "|AMZN_volume    |bigint   |       |\n",
      "|GOOG_price     |double   |       |\n",
      "|GOOG_volume    |bigint   |       |\n",
      "|MSFT_price     |double   |       |\n",
      "|MSFT_volume    |bigint   |       |\n",
      "|NVDA_price     |double   |       |\n",
      "|NVDA_volume    |bigint   |       |\n",
      "|TSLA_price     |double   |       |\n",
      "|TSLA_volume    |bigint   |       |\n",
      "|timestamp      |timestamp|       |\n",
      "|               |         |       |\n",
      "|# Partitioning |         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "Describing table: stock_news_html\n",
      "+------------------+---------+-------+\n",
      "|col_name          |data_type|comment|\n",
      "+------------------+---------+-------+\n",
      "|timestamp         |string   |null   |\n",
      "|AAPL              |bigint   |null   |\n",
      "|AMZN              |bigint   |null   |\n",
      "|GOOG              |bigint   |null   |\n",
      "|MSFT              |bigint   |null   |\n",
      "|NVDA              |bigint   |null   |\n",
      "|TSLA              |bigint   |null   |\n",
      "|hdfs_location     |string   |null   |\n",
      "|crawl_from_website|string   |null   |\n",
      "|html_text         |string   |null   |\n",
      "+------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all tables in the current database\n",
    "tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Describe each table\n",
    "for table in tables:\n",
    "    print(f\"Describing table: {table}\")\n",
    "    description = spark.sql(f\"DESCRIBE {table}\")\n",
    "    description.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
