{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0238fba-006c-42f4-9215-eac928c7d1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (52) Empty reply from server\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET http://172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8308e8-9432-4e89-a957-d480d8d322ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__consumer_offsets\n",
      "phongdinhcs-test-topic\n"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server 172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd17961b-b8f0-4eee-89b6-2e717b0cc516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while executing topic command : Topic 'phongdinhcs-test-topic' already exists.\n",
      "[2024-08-06 23:24:37,581] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'phongdinhcs-test-topic' already exists.\n",
      " (org.apache.kafka.tools.TopicCommand)\n"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-topics.sh --create --topic phongdinhcs-test-topic --bootstrap-server 172.18.0.99:9092 --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242bd3ca-b1cd-4b23-a0d0-59da0378d738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__consumer_offsets\n",
      "phongdinhcs-test-topic\n"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server 172.18.0.99:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452be777-fd9c-4289-a533-69c8e9b26286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consumer group 'my-group' has no active members.\n",
      "\n",
      "GROUP           TOPIC                  PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\n",
      "my-group        phongdinhcs-test-topic 0          1               20              19              -               -               -"
     ]
    }
   ],
   "source": [
    "!/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server 172.18.0.99:9092 --describe --group my-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e51531-30f9-448b-9892-a4d6f39fb5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message sent to topic 'phongdinhcs-test-topic'\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='172.18.0.99:9092',\n",
    "    value_serializer=lambda v: str(v).encode('utf-8')  # Convert messages to bytes\n",
    ")\n",
    "\n",
    "# Send a message to the topic\n",
    "producer.send('phongdinhcs-test-topic', value='Hello, Kafka!')\n",
    "\n",
    "# Wait for all messages to be sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message sent to topic 'phongdinhcs-test-topic'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c0ac2b-8c82-49ed-a400-4dd1c5978075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: Hello, Kafka!\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'phongdinhcs-test-topic',\n",
    "    bootstrap_servers='172.18.0.99:9092',\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: x.decode('utf-8')  # Decode bytes to string\n",
    ")\n",
    "\n",
    "# Consume messages from the topic\n",
    "for message in consumer:\n",
    "    print(f\"Received message: {message.value}\")\n",
    "    break  # Remove this break to continuously consume messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0890b4b6-a814-48f0-8429-31fa93cd7bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|teststocksymbols|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current database\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460abf8b-87fd-4825-b0de-a2b1e481a509",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/11 03:36:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\n",
      "Database: default\n",
      "----------------------------------------\n",
      "Table: teststocksymbols (Rows: 6)\n",
      "----------------------------------------\n",
      "Column Name     | Data Type  | Nullable   | Comment        \n",
      "--------------------------------------------------\n",
      "symbol          | string     |            | N/A            \n",
      "name            | string     |            | N/A            \n",
      "price           | double     |            | N/A            \n",
      "volume          | int        |            | N/A            \n",
      "                |            |            | N/A            \n",
      "# Partitioning  |            |            | N/A            \n",
      "Not partitioned |            |            | N/A            \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Describe all information in metastore\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Metastore Metadata\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Show all databases\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "\n",
    "# Loop through each database\n",
    "for db in databases:\n",
    "    db_name = db[0]  # Access the first column (database name)\n",
    "    spark.sql(f\"USE {db_name}\")\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    print(f\"\\nDatabase: {db_name}\\n\" + \"-\" * 40)\n",
    "    \n",
    "    # Loop through each table\n",
    "    for table in tables:\n",
    "        table_name = table[1]  # Access the second column (table name)\n",
    "        \n",
    "        # Count the number of rows in the table\n",
    "        row_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {table_name}\").collect()[0][0]\n",
    "        \n",
    "        print(f\"Table: {table_name} (Rows: {row_count})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Describe each table\n",
    "        description = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "        schema = description.schema\n",
    "        print(f\"{'Column Name':<15} | {'Data Type':<10} | {'Nullable':<10} | {'Comment':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for row in description.collect():\n",
    "            col_name = row[0]         # Column name\n",
    "            data_type = row[1]        # Data type\n",
    "            nullable = row[2]         # Exact nullable status\n",
    "            \n",
    "            # Handle optional comment field if present\n",
    "            if len(row) > 3:\n",
    "                comment = row[3]      # Column comment (if available)\n",
    "            else:\n",
    "                comment = \"N/A\"       # Default if no comment\n",
    "            \n",
    "            # Print with better formatting\n",
    "            print(f\"{col_name:<15} | {data_type:<10} | {nullable:<10} | {comment:<15}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0592acdc-b7ae-49b2-affe-859f1fc4ac31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching details for table: teststocksymbols in database: default\n",
      "\n",
      "Database: default\n",
      "\n",
      "Table: teststocksymbols\n",
      "format: delta\n",
      "id: 806ed304-8330-4e8e-87be-997af91087fb\n",
      "name: default.teststocksymbols\n",
      "description: None\n",
      "location: hdfs://namenode:8020/testlakehouse/TestStockSymbols\n",
      "createdAt: 2024-07-19 00:40:02.374000\n",
      "lastModified: 2024-08-06 23:23:24.342000\n",
      "partitionColumns: []\n",
      "numFiles: 6\n",
      "sizeInBytes: 7556\n",
      "properties: {}\n",
      "minReaderVersion: 1\n",
      "minWriterVersion: 2\n",
      "tableFeatures: ['appendOnly', 'invariants']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Delta configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TableDetailMetadata\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to get table details\n",
    "def get_table_details(database_name, table_name):\n",
    "    try:\n",
    "        details = spark.sql(f\"DESCRIBE DETAIL {database_name}.{table_name}\").collect()\n",
    "        if details:\n",
    "            detail_info = details[0].asDict()\n",
    "            return detail_info\n",
    "        else:\n",
    "            return {\"error\": \"Details not found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Get all databases\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "\n",
    "# Dictionary to hold metadata for all tables\n",
    "all_tables_metadata = {}\n",
    "\n",
    "for db in databases:\n",
    "    db_name = db.namespace\n",
    "    spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "    # Get all tables in the database\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    table_metadata = {}\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table.tableName\n",
    "        print(f\"Fetching details for table: {table_name} in database: {db_name}\")\n",
    "        \n",
    "        # Get table details\n",
    "        details = get_table_details(db_name, table_name)\n",
    "        table_metadata[table_name] = details\n",
    "\n",
    "    all_tables_metadata[db_name] = table_metadata\n",
    "\n",
    "# Display the metadata\n",
    "for db_name, tables in all_tables_metadata.items():\n",
    "    print(f\"\\nDatabase: {db_name}\")\n",
    "    for table_name, details in tables.items():\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "        for key, value in details.items():\n",
    "            print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657a168-7129-426e-bd11-86aa5883204e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
