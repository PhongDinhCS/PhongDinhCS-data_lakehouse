version: '3'

services: 
# build kafka container
  kafka:
    container_name: kafka
    image: apache/kafka:3.7.0  # Use the desired Kafka version
    ports:
      - "9092:9092"  # Expose Kafka port (default 9092)
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.99
        aliases:
          - kafka


#install hadoop hdfs docker container
#docker stop namenode datanode1 datanode2 resourcemanager nodemanager
#docker rm namenode datanode1 datanode2 resourcemanager nodemanager
#docker-compose up -d namenode datanode1 datanode2 resourcemanager nodemanager
#docker restart namenode datanode1 datanode2 resourcemanager nodemanager

  namenode:
    container_name: namenode
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
      - 8020:8020
    env_file:
      - ./hadoop/config
    environment:
      - ENSURE_NAMENODE_DIR=/hadoop/dfs/name
      - SPARK_SQL_CATALOG_IMPLEMENTATION=hive
      - HIVE_METASTORE_URIS=thrift://hive-metastore:9083
    volumes:
      - ./hadoop/dfs/name:/hadoop/dfs/name
      - hadoop-bin:/opt/hadoop # Share the hadoop bin directory
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.98
    # run this script in namenode after run docker-compose: docker exec -it namenode bash hdfs dfs -chmod 777 /

  datanode1:
    container_name: datanode1
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/dfs/data1:/hadoop/dfs/data
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.97

  datanode2:
    container_name: datanode2
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/dfs/data2:/hadoop/dfs/data
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.96

  resourcemanager:
    container_name: resourcemanager
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/test.sh:/opt/test.sh
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.94
    depends_on:
      - namenode
      - datanode1
      - datanode2

  nodemanager:
    container_name: nodemanager
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./hadoop/config
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.93
    depends_on:
      - resourcemanager

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: postgresql
    ports:
      - 5432:5432
    volumes:
      - postgresql-data:/var/lib/postgresql/data
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.91

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hive/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    ports:
      - 9083:9083
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.90

# Delta Spark container
  delta-spark:
    container_name: delta-spark
    image: deltaio/delta-docker:latest
    entrypoint:
      - bash
      - -c
      - |
        if ! hdfs dfs -test -d hdfs://namenode:8020/lakehouse; then
          hdfs dfs -mkdir hdfs://namenode:8020/lakehouse
          hdfs dfs -chmod 777 hdfs://namenode:8020/lakehouse
        fi
        tail -f /dev/null 
    volumes:
      - hadoop-bin:/opt/hadoop # Share the hadoop bin directory
      - ./delta-spark/volume:/opt/spark/work-dir/volume
      - ./delta-spark/conf:/opt/spark/conf
      - spark-data:/opt/spark/data
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - PATH=/opt/hadoop/bin:$PATH
      - HADOOP_USER_NAME=hdfs
      - SPARK_MODE=master
      - SPARK_SQL_CATALOG_IMPLEMENTATION=hive
      - HIVE_METASTORE_URIS=thrift://hive-metastore:9083
    networks:
      lakehouse_network:
        ipv4_address: 172.18.0.92
    ports:
      - "8888:8888"
      - "7077:7077"
      - "4040:4040"
    depends_on:
      - namenode
      - hive-metastore

networks:
  lakehouse_network:
    driver: bridge
    ipam:
     config:
       - subnet: 172.18.0.0/16
         gateway: 172.18.0.1

volumes:
  hadoop-bin:
  spark-data:
  postgresql-data: