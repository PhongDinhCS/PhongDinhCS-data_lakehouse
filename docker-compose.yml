version: '3'

services: 
# build kafka container
  kafka:
    container_name: kafka
    restart: unless-stopped
    image: apache/kafka:3.8.0
    ports:
      - 9092:9092  # Expose Kafka port (default 9092)
    volumes:
      - ./kafka/config:/mnt/shared/config
    networks:
      lakehouse:
        ipv4_address: 172.18.0.99

  namenode:
    container_name: namenode
    restart: unless-stopped
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
      - 8020:8020
      - 9000:9000
    env_file:
      - ./hadoop/config
    # environment:
    #   - ENSURE_NAMENODE_DIR=/hadoop/dfs/name
    volumes:
      - ./hadoop/dfs/name:/hadoop/dfs/name
      # - name:/hadoop/dfs/name
      - hadoop-bin:/opt/hadoop # Share the hadoop bin directory
    networks:
      lakehouse:
        ipv4_address: 172.18.0.98
    # run this script in namenode after run docker-compose: docker exec -it namenode bash hdfs dfs -chmod 777 /

  datanode1:
    container_name: datanode1
    restart: unless-stopped
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/dfs/data1:/hadoop/dfs/data
      # - data1:/hadoop/dfs/data
    networks:
      lakehouse:
        ipv4_address: 172.18.0.97

  datanode2:
    container_name: datanode2
    restart: unless-stopped
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/dfs/data2:/hadoop/dfs/data
      # - data2:/hadoop/dfs/data
    networks:
      lakehouse:
        ipv4_address: 172.18.0.96

  resourcemanager:
    container_name: resourcemanager
    restart: unless-stopped
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/test.sh:/opt/test.sh
    networks:
      lakehouse:
        ipv4_address: 172.18.0.94
    depends_on:
      - namenode
      - datanode1
      - datanode2

  nodemanager:
    container_name: nodemanager
    restart: unless-stopped
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./hadoop/config
    networks:
      lakehouse:
        ipv4_address: 172.18.0.93
    depends_on:
      - resourcemanager

  postgres:
    image: postgres:11
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hadoop'
      POSTGRES_PASSWORD: 'hadoop'
    ports:
      - '5432:5432'
    volumes:
      - postgresql:/var/lib/postgresql
    networks:
      lakehouse:
        ipv4_address: 172.18.0.91

  metastore:
    image: apache/hive:3.1.3
    depends_on:
      - postgres
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      HIVE_CONF_DIR: /opt/hive/conf
      DB_DRIVER: postgres
      # IS_RESUME: 'true' #importain: if you build this container the first time, let comment out this line #IS_RESUME: 'true', if restart the container let Resume true
      SERVICE_NAME: 'metastore'
    ports:
        - '9083:9083'
    volumes:
        - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
        - ./hadoop/core-site.xml:/opt/hive/conf/core-site.xml
        - ./hadoop/hdfs-site.xml:/opt/hive/conf/hdfs-site.xml
        - ./hadoop/yarn-site.xml:/opt/hive/conf/yarn-site.xml
        - type: bind
          source: ./hive/postgresql-42.2.27.jre6.jar
          target: /opt/hive/lib/postgres.jar
    networks:
      lakehouse:
        ipv4_address: 172.18.0.90

  hiveserver2:
    image: apache/hive:3.1.3
    depends_on:
      - metastore
    restart: unless-stopped
    container_name: hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      IS_RESUME: 'true'
      SERVICE_NAME: 'hiveserver2'
      HIVE_CONF_DIR: /opt/hive/conf
    ports:
      - '10000:10000'
      - '10002:10002'
    volumes:
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hadoop/core-site.xml:/opt/hive/conf/core-site.xml
      - ./hadoop/hdfs-site.xml:/opt/hive/conf/hdfs-site.xml
      - ./hadoop/yarn-site.xml:/opt/hive/conf/yarn-site.xml
    networks:
      lakehouse:
        ipv4_address: 172.18.0.89


# Delta Spark container
  delta-spark:
    container_name: delta-spark
    restart: unless-stopped
    image: deltaio/delta-docker:0.8.1_2.3.0
    entrypoint:
      - bash
      - -c
      - |
        if ! hdfs dfs -test -d hdfs://namenode:8020/lakehouse; then
          hdfs dfs -mkdir hdfs://namenode:8020/lakehouse
          hdfs dfs -chmod 777 hdfs://namenode:8020/lakehouse
        fi
        tail -f /dev/null 
    volumes:
      - hadoop-bin:/opt/hadoop # Share the hadoop bin directory
      - ./delta-spark/volume:/opt/spark/work-dir/volume
      - ./hive/conf/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./hadoop/core-site.xml:/opt/spark/conf/core-site.xml
      - ./hadoop/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml
      - ./hadoop/yarn-site.xml:/opt/spark/conf/yarn-site.xml
    environment:
      - DELTA_PACKAGE_VERSION=delta-core_2.12:2.3.0
      - SPARK_CONF_DIR=/opt/spark/conf
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - PATH=/opt/spark/bin:/opt/hadoop/bin:/opt/kafka/bin:$PATH
      - HADOOP_USER_NAME=hadoop
      - SPARK_MODE=master
      - SPARK_SQL_CATALOG_IMPLEMENTATION=hive
    networks:
      lakehouse:
        ipv4_address: 172.18.0.92
    ports:
      - "8888:8888"  # Jupyter Notebook
      - "7077:7077"  # Spark Master
      - "4040:4040"  # Spark Web UI
      - "5000:5000"  # Flask API
    depends_on:
      - namenode
# docker exec -it delta-spark bash -c "chmod +x /opt/spark/work-dir/startup.sh"
# docker exec -it delta-spark bash "/opt/spark/work-dir/startup.sh"

networks:
  lakehouse:
    name: lakehouse
    driver: bridge
    ipam:
     config:
       - subnet: 172.18.0.0/16
         gateway: 172.18.0.1

volumes:
  # name:
  # data1:
  # data2:
  hadoop-bin:
  postgresql:
  kafka: