
-----------------------------------------------------------------------------
// Local_host
docker stop namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker rm namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker-compose up -d namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker restart namenode datanode1 datanode2 resourcemanager nodemanager
docker start namenode datanode1 datanode2 resourcemanager nodemanager delta-spark
docker stop metastore postgres hiveserver2
docker rm metastore postgres hiveserver2
docker-compose up -d postgres metastore hiveserver2

docker exec -it hiveserver2 bash
/opt/hive/bin/schematool -dbType postgres -initSchema


// run this script to grant permission for other users can write to namenode hdfs 
// Local host
docker exec -it namenode bash hdfs dfs -chmod 777 /
-------------------------------------------------------------------------------

// we need to install Hadoop library inside Spark then Spark can use hdfs function.
// by download the release hadoop-X.Y.Z-src.tar.gz https://hadoop.apache.org/releases.html 
// extract the hadoop-X.Y.Z-src.tar.gz folder then rename the folder to hadoop.
//copy the folder to current project directory (the same folder with docker-compose)

docker cp hadoop delta-spark:opt/spark/work-dir

docker exec -it delta-spark bash

hadoop/bin/hdfs
export PATH=/opt/spark/work-dir/hadoop/bin:$PATH
echo $PATH
export HADOOP_USER_NAME=hdfs
echo $HADOOP_USER_NAME

hdfs dfs -mkdir hdfs://namenode:8020/lakehouse
hdfs dfs -ls hdfs://namenode:8020/

// we choose hdfs://namenode:8020/lakehouse directory inside hdfs is our lakehouse project home folder
// we should save others table inside this hdfs://namenode:8020/lakehouse folder

-------------------------------------------------------------------------------

// inside delta-spark
spark-shell --packages io.delta:delta-core_2.12:2.1.1


// scala
import org.apache.spark.sql.SparkSession
import io.delta.tables._

object DeltaOnHDFS {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Delta Lake on HDFS")
      .config("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.HDFSLogStore")
      .getOrCreate()

    // Create a DataFrame
    val data = Seq((1, "a"), (2, "b"), (3, "c")).toDF("id", "value")

    // Write DataFrame as a Delta Table
    data.write.format("delta").save("hdfs://namenode:8020/lakehouse/delta-table-test")

    spark.stop()
  }
}

DeltaOnHDFS.main(Array()) // Call the main method

----------------------------------------------------------------------------
// scala

import org.apache.spark.sql.SparkSession

object ReadDeltaFromHDFS {
  def main(args: Array[String]): Unit = {
    // Initialize SparkSession
    val spark = SparkSession.builder()
      .appName("Read Delta Table from HDFS")
      .config("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.HDFSLogStore")
      .getOrCreate()

    // Load Delta table
    val deltaDF = spark.read.format("delta").load("hdfs://namenode:8020/lakehouse/delta-table-test")

    // Show DataFrame
    deltaDF.show()

    // Stop SparkSession
    spark.stop()
  }
}

ReadDeltaFromHDFS.main(Array())

--------------------------------------------------------------------------
// Define a function to read the Delta table and sleep for 5 seconds
def loadAndSleep(): Unit = {
  ReadDeltaFromHDFS.main(Array())
  Thread.sleep(5000) // Sleep for 5 seconds
}

// Start the loop
while (true) {
  loadAndSleep()
}
------------------------------------------------------------
// Define a function to read the Delta table and sleep for 5 seconds
def ReadDeltaFromHDFSloadAndSleep(): Unit = {
  ReadDeltaFromHDFS.main(Array())
  Thread.sleep(5000) // Sleep for 5 seconds
}

// Start the loop
while (true) {
  ReadDeltaFromHDFSloadAndSleep()
}



---------------------------------------------------------------------------------
// git _ Local_host
git add .
git commit -m "take note this step"
git push origin main

-----------------------------------------------------------------------------------
// Local Host
docker cp ./backend/DeltaTableTestCreate.scala delta-spark:/opt/spark/work-dir/ 
docker cp ./backend/DeltaTableTestRead.scala delta-spark:/opt/spark/work-dir/ 
docker cp ./backend/DeltaTableTestReadAuto.scala delta-spark:/opt/spark/work-dir/

// scala
:load DeltaTableTestRead.scala
:load DeltaTableTestReadAuto.scala


:load DeltaTableStockPriceReadLoop.scala
:load DeltaTableStockPriceUpdateLoop.scala
---------
#Grand permission to execute script in delta-spark volume
# Local host
chmod -R 777 ./delta-spark/volume/
find ./delta-spark/volume -type f -exec chmod +x {} \;
ls -l ./delta-spark/volume/

docker exec -it delta-spark ./volume/DeltaTableStockSymbols.sh
docker exec -it delta-spark ./volume/DeltaTableStockPriceCreate.sh
docker exec -it delta-spark ./volume/DeltaTableStockPriceReadLoop.sh
docker exec -it delta-spark ./volume/DeltaTableStockPriceUpdateLoop.sh
-------
docker start namenode datanode1 datanode2 resourcemanager nodemanager delta-spark
docker restart namenode datanode1 datanode2 resourcemanager nodemanager delta-spark
-------
apt-get update
apt-get install -y openjdk-11-jdk
java -version
# Download the sbt launcher script
curl -sL "https://github.com/sbt/sbt/releases/download/v1.5.5/sbt-1.5.5.tgz" | tar zx -C /usr/local

# Create a symbolic link to sbt
ln -s /usr/local/sbt/bin/sbt /usr/bin/sbt
sbt sbtVersion
cd /opt/spark/work-dir/volume/sbt
sbt package

spark-submit --class DeltaTableStockPriceReadLoop --master local[2] \
  --packages io.delta:delta-core_2.12:1.0.0 \
  /opt/spark/work-dir/volume/sbt/target/scala-2.12/deltastockpricereadloop_2.12-1.0.jar

------------------
pip install pyspark
pip install jupyter
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''

-----------------
echo 'export PATH=$PATH:/usr/local/openjdk-11/bin' >> ~/.bashrc
source ~/.bashrc
------
spark-sql --packages io.delta:delta-core_2.12:2.1.1
--------------
docker exec -it postgresql bash

--------------
./startup.sh 
