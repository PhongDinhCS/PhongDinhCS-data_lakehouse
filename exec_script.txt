// Local_host
docker stop namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker rm namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker-compose up -d namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker restart namenode datanode datanode2 datanode3 resourcemanager nodemanager

// run this script to grant permission for other users can write to namenode hdfs 
// Local host
docker exec -it namenode bash hdfs dfs -chmod 777 /
-------------------------------------------------------------------------------

// we need to install Hadoop library inside Spark then Spark can use hdfs function.
// by download the release hadoop-X.Y.Z-src.tar.gz https://hadoop.apache.org/releases.html 
// extract the hadoop-X.Y.Z-src.tar.gz folder then rename the folder to hadoop.
//copy the folder to current project directory (the same folder with docker-compose)

docker cp hadoop delta-spark:opt/spark/work-dir

docker exec -it delta-spark bash

hadoop/bin/hdfs
export PATH=/opt/spark/work-dir/hadoop/bin:$PATH
export HADOOP_USER_NAME=hdfs

hdfs dfs -mkdir hdfs://namenode:8020/lakehouse

// we choose hdfs://namenode:8020/lakehouse directory inside hdfs is our lakehouse project home folder
// we should save others table inside this hdfs://namenode:8020/lakehouse folder

-------------------------------------------------------------------------------

// inside delta-spark
spark-shell --packages io.delta:delta-core_2.12:2.1.1


// scala
import org.apache.spark.sql.SparkSession
import io.delta.tables._

object DeltaOnHDFS {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Delta Lake on HDFS")
      .config("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.HDFSLogStore")
      .getOrCreate()

    // Create a DataFrame
    val data = Seq((1, "a"), (2, "b"), (3, "c")).toDF("id", "value")

    // Write DataFrame as a Delta Table
    data.write.format("delta").save("hdfs://namenode:8020/lakehouse/delta-table-test")

    spark.stop()
  }
}

DeltaOnHDFS.main(Array()) // Call the main method

----------------------------------------------------------------------------
// scala

import org.apache.spark.sql.SparkSession

object ReadDeltaFromHDFS {
  def main(args: Array[String]): Unit = {
    // Initialize SparkSession
    val spark = SparkSession.builder()
      .appName("Read Delta Table from HDFS")
      .config("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.HDFSLogStore")
      .getOrCreate()

    // Load Delta table
    val deltaDF = spark.read.format("delta").load("hdfs://namenode:8020/lakehouse/delta-table-test")

    // Show DataFrame
    deltaDF.show()

    // Stop SparkSession
    spark.stop()
  }
}

ReadDeltaFromHDFS.main(Array())

--------------------------------------------------------------------------
// Define a function to read the Delta table and sleep for 5 seconds
def loadAndSleep(): Unit = {
  ReadDeltaFromHDFS.main(Array())
  Thread.sleep(5000) // Sleep for 5 seconds
}

// Start the loop
while (true) {
  loadAndSleep()
}
------------------------------------------------------------
// Define a function to read the Delta table and sleep for 5 seconds
def ReadDeltaFromHDFSloadAndSleep(): Unit = {
  ReadDeltaFromHDFS.main(Array())
  Thread.sleep(5000) // Sleep for 5 seconds
}

// Start the loop
while (true) {
  ReadDeltaFromHDFSloadAndSleep()
}



---------------------------------------------------------------------------------
// git _ Local_host
git add .
git commit -m "take note this step"
git push origin main

-----------------------------------------------------------------------------------
// Local Host
docker cp ./backend/DeltaTableTestCreate.scala delta-spark1:/opt/spark/work-dir/ 
docker cp ./backend/DeltaTableTestRead.scala delta-spark1:/opt/spark/work-dir/ 
docker cp ./backend/DeltaTableTestReadAuto.scala delta-spark1:/opt/spark/work-dir/

// scala
:load DeltaTableTestRead.scala
:load DeltaTableTestReadAuto.scala


:load DeltaTableStockPriceReadLoop.scala
:load DeltaTableStockPriceUpdateLoop.scala
---------

